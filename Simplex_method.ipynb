{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in file\n",
    "df = spark.read.csv(\"C:/Users/290002494/Documents/Python Scripts/Data Science/Pokemon.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Stage|count|\n",
      "+-----+-----+\n",
      "|    1|   79|\n",
      "|    3|   16|\n",
      "|    2|   56|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Stage\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def large_data_plot(data_frame, column_sample_by = \"\", num_points = 100, sample_ratio = [], cols = []):\n",
    "    \n",
    "    #if column_sample_by is blank then just sample based on num_points\n",
    "    \n",
    "    #sample_ratio should add up to 1\n",
    "    #num_points is the total number of points you want in the final dataset\n",
    "    #column_sample_by is the column you want to stratify\n",
    "    #cols is to select only necessary columns\n",
    "    \n",
    "    #Method is robust to theoretically work with any number of categories binary to many\n",
    "    \n",
    "    #import necessary modules\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "    from scipy.optimize import linprog\n",
    "\n",
    "\n",
    "    if column_sample_by != \"\":\n",
    "\n",
    "        unique_value_length = len(sample_ratio)\n",
    "\n",
    "        #get our totals of each by variable occurence\n",
    "        pd_frame = data_frame.select(column_sample_by).groupBy(column_sample_by).count().orderBy(column_sample_by).select(column_sample_by,\"count\").toPandas()\n",
    "\n",
    "        #get list of unique values and their total count\n",
    "        sampling_variable = list(pd_frame[column_sample_by])\n",
    "        sampling_totals = list(pd_frame[\"count\"])\n",
    "\n",
    "        #get all possible constraint combinations\n",
    "        columns = []\n",
    "        for i in range(unique_value_length):\n",
    "            if i < 2:\n",
    "                pass\n",
    "            else:\n",
    "                columns.append(list(itertools.combinations(range(unique_value_length),i)))\n",
    "\n",
    "        columns = [part for item in columns for part in item]\n",
    "        #if binary make the list empty\n",
    "        if unique_value_length < 3:\n",
    "            columns = []\n",
    "\n",
    "        ###set up format for simplex method\n",
    "        #3 parts to our matrix\n",
    "\n",
    "        #main constraint => + 1\n",
    "        #relationship inequalities => n(n-1)/2. number of rows for relationship constraints\n",
    "        #combinations => len(columns)\n",
    "\n",
    "        #add all 3 to get the number of rows for our matrix\n",
    "        #length of each array needs to add unique_value_length because each inequality has a slack variable\n",
    "\n",
    "        sum_formula = ((unique_value_length)*(unique_value_length -1))/2\n",
    "        num_arrays_needed = int(sum_formula + len(columns) + 1)\n",
    "        length_of_each_array = unique_value_length + num_arrays_needed\n",
    "\n",
    "        #initialize our linear program\n",
    "        program_array = np.zeros((num_arrays_needed, length_of_each_array))\n",
    "\n",
    "\n",
    "        #minimization function\n",
    "        program_array[0,range(unique_value_length)] = -1\n",
    "\n",
    "        c = program_array[0, :].copy()\n",
    "\n",
    "\n",
    "        #reset our first row\n",
    "        program_array[0,range(unique_value_length)] = 1   \n",
    "        program_array[0, unique_value_length] = 1 #the slack variable in the first/main inequality\n",
    "\n",
    "\n",
    "\n",
    "        #make the constraints\n",
    "\n",
    "        for i in range(1,num_arrays_needed): #skip the first row as it was a special inequality that we already made\n",
    "            #add slack terms\n",
    "            program_array[i,i+unique_value_length] = 1\n",
    "\n",
    "            if i in list(range(1,len(columns) + 1)):\n",
    "                program_array[i,columns[i-1]] = 1\n",
    "                #need to make relationship inequalities for the ones with 2 value column lengths\n",
    "                if len(columns[i-1]) == 2:\n",
    "                    program_array[i+len(columns),columns[i-1]] = 1\n",
    "\n",
    "        if unique_value_length < 3:\n",
    "            program_array[1,0:2] = 1\n",
    "\n",
    "\n",
    "        #make b matrix\n",
    "        b = np.zeros(num_arrays_needed)\n",
    "        b[0] = num_points\n",
    "\n",
    "        for i in range(len(b)):\n",
    "            if i not in list(range(0,len(columns)+1)):\n",
    "                b[i] = 0\n",
    "            elif i == 0:\n",
    "                pass\n",
    "            else:\n",
    "                value_holder = sum([sample_ratio[i] for i in columns[i-1]])\n",
    "                b[i] = num_points*value_holder\n",
    "\n",
    "        #make the final few rows in the matrix A...\n",
    "        for i in range(len(b)):\n",
    "            if b[i] == 0:#i not in list(range(0,len(columns)+1)):\n",
    "                index_value_last = np.nonzero(program_array[i,:unique_value_length])[0][-1]\n",
    "                index_value_first = np.nonzero(program_array[i,:unique_value_length])[0][0]\n",
    "                #add the constraint values correctly\n",
    "                program_array[i, index_value_last] = \\\n",
    "                -(sample_ratio[index_value_first])/(sample_ratio[index_value_last])\n",
    "\n",
    "        #print(c)\n",
    "        #print(np.round(program_array,2))\n",
    "        print(b)\n",
    "\n",
    "\n",
    "        #set up and run simplex method\n",
    "        bound_list = []\n",
    "\n",
    "        #set up bounds for slack variables of the inequalities\n",
    "        for i in range(length_of_each_array):\n",
    "\n",
    "            if i < unique_value_length:\n",
    "\n",
    "                #can't go over the maximum number of points in that category that exist\n",
    "                bound_list.append((1,min(num_points*sample_ratio[i], sampling_totals[i])))\n",
    "\n",
    "            elif (i >= unique_value_length) & (i < 1 + unique_value_length + len(columns)):\n",
    "                bound_list.append((0,10000))\n",
    "            else:\n",
    "                bound_list.append((0,1))\n",
    "        #make a tuple \n",
    "        tuple_bounds = tuple(bound_list)\n",
    "\n",
    "        #solve our equations\n",
    "        res = linprog(c, A_eq = program_array, b_eq = b, \n",
    "                      bounds = tuple_bounds,\n",
    "                 method = \"simplex\")\n",
    "        print(res.x[:unique_value_length])\n",
    "\n",
    "        #return our values\n",
    "        numbers_per_category =  np.round(res.x[:unique_value_length],0)\n",
    "        percentages = np.round(numbers_per_category/sampling_totals,2)\n",
    "\n",
    "        #now retrieve the dataset\n",
    "        fractions = {}\n",
    "        for i in range(unique_value_length):\n",
    "            fractions[sampling_variable[i]] = percentages[i]\n",
    "\n",
    "\n",
    "        sampled_df = data_frame.sampleBy(column_sample_by, fractions = fractions)\n",
    "    \n",
    "    else:\n",
    "        percentages = np.round(num_points/data_frame.count(),2)\n",
    "        sampled_df = data_frame.sample(False, percentages)\n",
    "        print(percentages)\n",
    "        print(sampled_df.count())\n",
    "        \n",
    "    #select necessary columns\n",
    "    if cols == []:\n",
    "        pass\n",
    "    else:\n",
    "        #fix columns with spaces or periods in their names\n",
    "        new_cols = []\n",
    "        for i in sampled_df.columns:\n",
    "            sampled_df = sampled_df.withColumnRenamed(i, i.replace(\" \", \"_\").replace(\".\", \"\"))\n",
    "        col_list = cols\n",
    "        #fix in case variables have a space in their name only for the variables required\n",
    "        col_list = [i.replace(\" \", \"_\").replace(\".\", \"\") for i in col_list]\n",
    "        sampled_df.select([col for col in sampled_df.columns if col in col_list])\n",
    "    \n",
    "    return sampled_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.  70.  50.  60.   0.   0.   0.]\n",
      "[24. 32. 16.]\n"
     ]
    }
   ],
   "source": [
    "large_data_plot(data_frame = df, num_points = 100, column_sample_by = \"Stage\", sample_ratio = [.3,.4,.2], cols = []).to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! bokeh serve --show Simplex_Dashboard.ipynb --port 5004\n",
    "#runs dashboard"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
